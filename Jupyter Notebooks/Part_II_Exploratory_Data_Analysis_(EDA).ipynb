{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part II - Exploratory Data Analysis (EDA).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFb0HOi6VQ0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# links:\n",
        "# https://github.com/akshaybhatia10/Reddit-Flair-Detection\n",
        "# https://github.com/abhishek-parashar/Reddit-flair-detection/blob/master/experiment_log_for_eda.ipynb\n",
        "# https://github.com/abhishekchopra0907/Reddit-Flair-Detector/blob/master/Jupyter%20Notebooks/data%20analysis.ipynb\n",
        "# complete all the parts:\n",
        "=========================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgADD-woYmP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Part II - Exploratory Data Analysis (EDA)\n",
        "Perform EDA on the data collected in Part I. This is helpful for understanding the data you have\n",
        "collected. Explain in detail about the analysis you did, intuition behind doing it, output of the\n",
        "analysis (in terms of graphs or tables), your inference from the output and how it shapes your\n",
        "future system decisions."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gNbJZZM_Vy",
        "colab_type": "text"
      },
      "source": [
        "Exploratory Data Analysis (EDA):\n",
        "---\n",
        "Exploratory Data Analysis is a process of :-\n",
        "* understanding the data\n",
        "* building intuition about the data\n",
        "* generating and  testing hypothesis about data\n",
        "* and finding insights in the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxdi66WOYtyS",
        "colab_type": "text"
      },
      "source": [
        "Our Aim:\n",
        "---\n",
        "In this part, we will first try to verify assumption of **Part I - Reddit Data Collection**:\n",
        "* whether feature collected of a post relates to flair of a post\n",
        "* then, we will try to find insight in the data\n",
        "* and at last, try to generate new features.\n",
        "\n",
        "**Note: This part has been divided into three sections:**\n",
        "1. Univariate Analysis: to explore individual features\n",
        "2. Bivariate Analysis: to explore feature relations\n",
        "3. Feature Groups: to explore how can we combine them to get high accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3f3GpDa_skL",
        "colab_type": "text"
      },
      "source": [
        "Univariate Analysis:\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NzpCXEHazTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of visualization tools to use:\n",
        "# plt.hist(x)\n",
        "# plt.plot(x, '.') # x axis row index, y axis feature values\n",
        "# plt.scatter(range(len(x)), x, c=y)\n",
        "# df.describe\n",
        "# x.mean()\n",
        "# x.var()\n",
        "# x.value_counts()\n",
        "# x.isnull()\n",
        "# plt.scatter(x1, x2)\n",
        "# pd.scatter_matrix(df)\n",
        "# df.corr() plt.matshow()\n",
        "# df.mean().sort_values().plot(style='.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qal6tECwecb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# exploring anonymized data\n",
        "# df.dtypes()\n",
        "# df.info()\n",
        "# x.value_counts()\n",
        "# x.isnull()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Xb51aGz96E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset cleaning and other things to check\n",
        "# a feature takes same value for every object in both train and test set.\n",
        "# traintest.nunique(axis=1) == 1\n",
        "# train.nunique(axis=1) == 1\n",
        "# duplicated numerical features\n",
        "# traintest.T.drop_duplicates()\n",
        "# for f in categorical_feats:\n",
        "# traintest[f] = traintest[f].factorize()\n",
        "# traintest.T.drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9E48cy_eAIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first find dataset in which we can do eda\n",
        "# go through notebook and see if \n",
        "# they try to do find features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omjtz2wg6iaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/abhisheksaxena1998/Reddit-Flair-Detector-Application/blob/master/Jupyter%20Notebooks/Part%20II%20-%20Exploratory%20Data%20Analysis%20(EDA).ipynb\n",
        "# https://github.com/abhisheksaxena1998/Reddit-Flair-Detector/blob/master/reddit_stats.ipynb\n",
        "# https://github.com/anujarora999/reddit_flare_detection/blob/master/EDA_reddit_flair.ipynb\n",
        "# https://github.com/azf99/midas-iiitd-internship-challenge-2020/blob/master/Part%202-%20Exploratory%20Data%20Analysis.ipynb\n",
        "# https://github.com/keshav-b/Reddit-Flair-Detection/blob/master/Models/Part%20II%20-%20Exploratory%20Data%20Analysis%20(EDA).ipynb\n",
        "# https://github.com/Mihir-Rajora20/Reddit-Flair-Detector/blob/master/Data%20Exploration%20and%20Modification.ipynb\n",
        "# https://github.com/Mihir-Rajora20/Reddit-Flair-Detector/blob/master/Data%20Extraction%20Using%20Pushshifts.ipynb\n",
        "# https://github.com/sawarn69/Reddit-Flair-Detection/blob/master/Download%20data%2B%20EDA.ipynb\n",
        "# https://github.com/Shaurya-L/Reddit-Flair-Detector/blob/master/Data%20Analysis.ipynb\n",
        "# https://nbviewer.jupyter.org/github/someshsingh22/FlaiReddit-MIDAS/blob/master/Part-2-EDA.ipynb\n",
        "# https://github.com/yatharthmathur/RedditAnalysis/blob/master/EDA/EDA.ipynb\n",
        "# https://github.com/yatharthmathur/RedditAnalysis/blob/master/EDA/Visualization.ipynb\n",
        "\n",
        "# use this repo\n",
        "# https://github.com/akshaybhatia10/Reddit-Flair-Detection\n",
        "# https://github.com/harshita219/Reddit-Flair-Detection\n",
        "# https://github.com/Himanshu-Garg/Reddit-flair-detection\n",
        "# https://github.com/Mihir-Rajora20/Reddit-Flair-Detector # for readme\n",
        "# important ones\n",
        "# https://github.com/manya16400/Reddit-Flair-Detection/blob/master/Notebooks/1.Data%20Scraping.ipynb\n",
        "# https://github.com/kshitijgulati98/reddit-flair-detector/blob/master/Jupyter%20Notebooks/Part%202%20-%20EDA.ipynb\n",
        "# https://github.com/Himanshu-Garg/Reddit-flair-detection/blob/master/other/1year_data.ipynb\n",
        "# https://github.com/harshita219/Reddit-Flair-Detection/blob/master/EDA.ipynb\n",
        "# https://github.com/harshita219/Reddit-Flair-Detection/blob/master/extract_data.ipynb\n",
        "# https://github.com/akshaybhatia10/Reddit-Flair-Detection/blob/master/notebooks/data_aquisition.ipynb\n",
        "# https://github.com/akshaybhatia10/Reddit-Flair-Detection/blob/master/app/app.py\n",
        "# https://github.com/BhavyaC16/FlairifyMe/blob/master/app.py\n",
        "# https://github.com/parasmehan123/RedFlair/blob/master/model.py\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2nemRtvMiBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMDnkOKI1aCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('data.csv')\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCfNIDdhWAql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.selftext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXkbSr9I063L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZpYrGtt1ksB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihU80OEG3Czf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['created_utc'] = pd.to_datetime(data['created_utc'], unit='s')\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k85OqrFz5FHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpyIFweN1gfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (15, 5)\n",
        "data.hist();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeJhf5Jm2WRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.created_utc.value_counts().sort_values()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAA6vTNs2M7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(data.created_utc, bins=30);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqHyz_SX6ghF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.num_comments.value_counts().sort_values()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzKzi9Wg7InL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(data.num_comments, bins=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoLAJm-NAn4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "hist, bin_edges = np.histogram(data.num_comments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHgzUJniBA0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNzMaLRMBPQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bin_edges[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy3A-95HC1Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min(bin_edges)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ydAy-QC2rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max(bin_edges)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdyInzYhBB1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMpdISYGCx7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.bar(bin_edges[:-1], hist, width = 0.5, color='#0504aa',alpha=0.7)\n",
        "plt.xlim(min(bin_edges), max(bin_edges))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhTyRUpYBGwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[10,8])\n",
        "\n",
        "plt.bar(bin_edges[:-1], hist, width = 0.5, color='#0504aa',alpha=0.7)\n",
        "plt.xlim(min(bin_edges), max(bin_edges))\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.xlabel('Value',fontsize=15)\n",
        "plt.ylabel('Frequency',fontsize=15)\n",
        "plt.xticks(fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.ylabel('Frequency',fontsize=15)\n",
        "plt.title('Normal Distribution Histogram',fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gIs5bCmBrDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(100)\n",
        "np_hist = np.random.normal(loc=0, scale=1, size=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ltm2gAzBunr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist,bin_edges = np.histogram(np_hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GeJd2HfB3rW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bin_edges[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy1TQlDk09PB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# to modify \n",
        "plt.rcParams['figure.figsize'] = (15, 5)\n",
        "data.link_flair_text.hist(bins=20);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnG99KSL1ofS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making sure data is intuitive: 1000 observations and 11 features\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk5Y1INo17MB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# to modify \n",
        "plt.rcParams['figure.figsize'] = (15, 5)\n",
        "data.link_flair_text.hist(bins=20);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XomCWz2_Erwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNi84JgjLEqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(data.author, data.created_utc, c=data.link_flair_text.factorize()[0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIoOk0UkLPZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode text to numeric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7pfZlMLLml4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.plotting.scatter_matrix(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxrLCMieL7i9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.corr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CXwgbv2L9DA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.matshow()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY97OdiQMcRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.mean().sort_values().plot(style='.');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogN2SN2OMjCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8vlUXJeJv3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(data.created_utc, data.score, c=data.link_flair_text.factorize()[0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y_x8D_xF-Ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.plot(data.over_18, '.');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoEBsxckI4KJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "# plot index versus value\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.scatter(range(len(data.over_18)), data.over_18, c=data.link_flair_text.factorize()[0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU_rAB_8F7js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.plot(data.score, '.');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jnBgaH3IxWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "# plot index versus value\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.scatter(range(len(data.score)), data.score, c=data.link_flair_text.factorize()[0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "282cktCSF0_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.plot(data.num_comments, '.');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdluxTkTIjpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "# plot index versus value\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.scatter(range(len(data.num_comments)), data.num_comments, c=data.link_flair_text.factorize()[0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3oB-oYUFxPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.plot(data.link_flair_text, '.');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBZPFlTzFrOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.plot(data.created_utc, '.');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1N6-UeOIUBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "# plot index versus value\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.scatter(range(len(data.created_utc)), data.created_utc, c=data.link_flair_text.factorize()[0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkw2_SzEEguP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "plt.figure(figsize=[30,30])\n",
        "plt.plot(data.author, '.');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsCJkp-eEWKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x-axis row index, y-axis feature value\n",
        "# plot index versus value\n",
        "plt.figure(figsize=[30,30])\n",
        "plt.scatter(range(len(data.author)), data.author, c=data.link_flair_text.factorize()[0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByEWp796PbmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature extraction and preprocessing with respect to models\n",
        "# bag of words\n",
        "# embeddings (word to vector)\n",
        "# tfidf\n",
        "# N-grams\n",
        "# lowercase\n",
        "# lemmatization\n",
        "# stemming\n",
        "# stopwords\n",
        "# word2vec glove, fastText, doc2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceei5fKnTe4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zydqAJaA28XD",
        "colab_type": "text"
      },
      "source": [
        "Exploring individual features:\n",
        "---\n",
        "We will first explore individual features and then we will try to explore their relations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUkE9MIi2ESs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.nunique().sort_values()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77NtYN133Xjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.selftext.value_counts().sort_values()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppV-A2bu3tnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/abhishekchopra0907/Reddit-Flair-Detector/master/reddit-india-data.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tckdLBx7KdMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#downloading libraries\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "#nltk.download()\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSmp37YjLFCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Dropout, Flatten  \n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTy3nwIKKLMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv('reddit-india-data.csv')\n",
        "data=data.fillna('NO VALUE')\n",
        "feature_combine = data[\"title\"] + data[\"comments\"]\n",
        "data = data.assign(feature_combine = feature_combine)\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe-bc_EeKWqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_extractor(text,text_type):\n",
        "    title_list=[]\n",
        "    for i in range(len(text)):\n",
        "        title_list.append(text[text_type][i])\n",
        "    return title_list\n",
        "\n",
        "def preProcessData(dataa):\n",
        "    stopwords_en = list(set(stopwords.words('english')))\n",
        "    def split(word): \n",
        "        return [char for char in word]   \n",
        "    punchList = split(punctuation)\n",
        "\n",
        "    print(stopwords_en)\n",
        "    print('Punctuation :', punchList)\n",
        "\n",
        "    wordTokenList = [word_tokenize(sent) for sent in dataa]\n",
        "    lowercasingList = [[word.lower() for word in sentence] for sentence in wordTokenList]\n",
        "    noStopWordList = [[word for word in sentence if word not in stopwords_en] for sentence in lowercasingList]\n",
        "    noPunchList = [[re.sub(r'([^\\s\\w]|_)+', '', word) for word in sentence] for sentence in noStopWordList]\n",
        "    #noPunchList = [[word for word in sentence if word not in punchList] for sentence in noStopWordList]\n",
        "    PP_data = [[word for word in sentence if word] for sentence in noPunchList]\n",
        "    return PP_data\n",
        "\n",
        "def cleaner(PP_data):\n",
        "    PP_corr=[]\n",
        "    for title in PP_data:\n",
        "        temp=[]\n",
        "        for word in title:\n",
        "            if word in embeddings_index.keys():\n",
        "                    temp.append(word)\n",
        "        PP_corr.append(temp)\n",
        "    return PP_corr\n",
        "\n",
        "def pad_features(titles,max_len):\n",
        "    features=[]\n",
        "    for title in titles:\n",
        "        temp=[]\n",
        "        for i in range(max_len):\n",
        "            if i < len(title):\n",
        "                temp.append(title[i])\n",
        "            else:\n",
        "                temp.append(\"ignore_nan\")\n",
        "        features.append(temp)\n",
        "    return features\n",
        "\n",
        "\n",
        "def vectorizer(summ_data,embeddings_index):\n",
        "    summ_vec=[]\n",
        "    for list_w in summ_data:\n",
        "        temp=[]\n",
        "        for word in list_w:\n",
        "            if word in embeddings_index.keys():\n",
        "                temp.append(embeddings_index[word])\n",
        "        summ_vec.append(temp)\n",
        "    return summ_vec\n",
        "\n",
        "def sent_vectorizer(vector):\n",
        "    sent_vect=[]\n",
        "    for sent in vector:\n",
        "        n=0\n",
        "        temp=np.zeros(200)\n",
        "        for word in sent:\n",
        "            temp=temp+word\n",
        "            n=n+1\n",
        "        sent_vect.append(temp/n)\n",
        "    return sent_vect\n",
        "'''\n",
        "import os\n",
        "glove_dir = 'glove'\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "z=np.zeros(200)\n",
        "embeddings_index['ignore_nan']=z\n",
        "'''\n",
        "def reshaper(t_x):\n",
        "    x = np.reshape(t_x, (t_x.shape[0], 1,t_x.shape[1]))\n",
        "    return x\n",
        "\n",
        "def data_to_ready(data,embed):\n",
        "    t_list=text_extractor(data,'title')\n",
        "    pp_data=preProcessData(t_list)\n",
        "    clean=cleaner(pp_data)\n",
        "#     max_len=max_finder(clean)\n",
        "    clean=pad_features(clean,256)\n",
        "    final=vectorizer(clean,embed)\n",
        "    final=sent_vectorizer(final)\n",
        "    final=np.array(final)\n",
        "#     re_final=reshaper(final)\n",
        "#     return re_final\n",
        "    return final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaQkeAEjKoo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffler(data):\n",
        "    df_sh=shuffle(data)\n",
        "    df_sh=df_sh.reset_index()\n",
        "    return df_sh\n",
        "def one_shotter(z,colt):\n",
        "    y=z[colt]\n",
        "    y=np.array(y)\n",
        "    y=pd.get_dummies(y)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAs-8gaSK6fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sh = shuffler(data)\n",
        "sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqoDWmjLK8TQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPeKoKe-M0n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/akshala/reddit-flair-detector/master/initial_final.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9yVsICnOG2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean(text):\n",
        "\ttext = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "\ttext = text.lower() # lowercase text\n",
        "\ttext = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "\ttext = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "\ttext = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTx3Ng8XOUdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "\n",
        "data = pandas.read_csv('initial_final.csv')\n",
        "\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj5PIEbEOd2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[['title']] = data.apply(lambda x: clean(x['title']), axis=1)\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp8KYj39PU8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[['comments']] = data.apply(lambda x: clean(x['comments']), axis=1)\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoUNpUnvPZRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = data['link_flair_text']\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLSzINcuS8AS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flairs = list(set(y))\n",
        "\n",
        "flairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jae9UU6S-YM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data['title'] + data['url'] + data['comments'] + data['author'].fillna('') + data['id']\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTmiqgrGTYBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY0r2SB1TC_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
        "X_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpJseE5GTidG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import preprocessing\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rAzJVOvTL4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logreg = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression(n_jobs=1, C=1e5)),])\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24oxf9hrTeAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = logreg.predict(X_test)\n",
        "print('accuracy (combined)', accuracy_score(y_pred, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2WQatGgT8om",
        "colab_type": "text"
      },
      "source": [
        "Bag-of-Words:\n",
        "---\n",
        "The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O21g3TT_TxNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# basic preprocessing:\n",
        "\n",
        "import nltk\n",
        "def to_lower(text):\n",
        "  \"\"\"\n",
        "  Converting text to lower case as in converting \"HELLO\" to \"hello\"\n",
        "  \"\"\"\n",
        "  return ' '.join([w.lower() for w in word_tokenize(text)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LuC88XhXT0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature extraction from text\n",
        "bag of words\n",
        "embeddings word 2 vec\n",
        "coutvectorizer sklearn\n",
        "term frequency\n",
        "inverse document \n",
        "text.tfidfvectorizer sklearn\n",
        "n-grams \n",
        "sklearn ngram_range, analyzer\n",
        "text preprocessing\n",
        "lowercase\n",
        "countvectorizer from sklearn does this by default\n",
        "lemmatization advanced preprocessing\n",
        "stemming\n",
        "stopwords\n",
        "sklearn.feature_extraction.text.countvectorizer\n",
        "max_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3DMUU0_dELs",
        "colab_type": "text"
      },
      "source": [
        "Stemming:\n",
        "---\n",
        "the process of removing prefixes and suffixes from words until we are left with its stem, which carries most of its meaning. So cars is stemmed to car"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgkjHiTSgDjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OefEXm8BgGFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# different stemmers available in different languages in Python nltk\n",
        "# for the English language, you can choose between PorterStammer or LancasterStammer\n",
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxz_dePRmQpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgaxtqLgmacA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "# provide a word to be stemmed\n",
        "print(\"Porter Stemmer\")\n",
        "print(porter.stem(\"cats\"))\n",
        "print(porter.stem(\"trouble\"))\n",
        "print(porter.stem(\"troubling\"))\n",
        "print(porter.stem(\"troubled\"))\n",
        "print(\"Lancaster Stemmer\")\n",
        "print(lancaster.stem(\"cats\"))\n",
        "print(lancaster.stem(\"trouble\"))\n",
        "print(lancaster.stem(\"troubling\"))\n",
        "print(lancaster.stem(\"troubled\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K690Ws2gm4RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PorterStemmer use Suffix Stripping to produce stems\n",
        "# PorterStemmer algorithm does not follow linguistics rather a set of 05 rules\n",
        "# for different cases that are applied in phases (step by step) to generate\n",
        "# stems"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZEe2uRmnbIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A list of words to be stemmed\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"stabil\", \"destabilize\", \"misunderstanding\", \"railroad\", \"moonlight\", \"football\"]\n",
        "print(\"{0:20}{1:20}{2:20}\".format(\"Word\", \"Porter Stemmer\", \"lancaster Stemmer\"))\n",
        "for word in word_list:\n",
        "  print(\"{0:20}{1:20}{2:20}\".format(word, porter.stem(word), lancaster.stem(word)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdM-RUD1oCgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The LancasterStemmer (Paice-Husk stemmer) is an iterative algorithm with\n",
        "# rules saved externally. One table containing about 120 rules indexed by the\n",
        "# last letter of a suffix. On each iteration, it tries to find an applicable\n",
        "# rule by the last character of the word."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHJqfzEYout1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
        "porter.stem(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGE8DroBo9Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stemmers sees the entire sentence as a word, so it returns it as it is.\n",
        "# We need to stem each word in the sentence and return a combined sentence.\n",
        "# to separate the sentence into words, you can use tokenizer."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zbV6l11pcbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def stemSentence(sentence):\n",
        "  token_words = word_tokenize(sentence)\n",
        "  token_words\n",
        "  stem_sentence = []\n",
        "  for word in token_words:\n",
        "    stem_sentence.append(porter.stem(word))\n",
        "    stem_sentence.append(\" \")\n",
        "  return \"\".join(stem_sentence)\n",
        "\n",
        "x = stemSentence(sentence)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIGLKrispcWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcMu2zCypcRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UVTWYEFpcMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_file = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
        "my_lines_list = []\n",
        "for line in text_file:\n",
        "  my_lines_list.append(line)\n",
        "\n",
        "my_lines_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dSoWiPgrPSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_file = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
        "text_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhW6ebKWre19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_lines_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlPMTm-cr11G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SnowballStemmer as a language to create to non-English stemmers. One\n",
        "# can program one's own language stemmer using snowball."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANHclNr9sNbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "englishStemmer = SnowballStemmer(\"english\")\n",
        "englishStemmer.stem(\"having\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsNq59jSsU3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjQTScqVsdAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "englishStemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "englishStemmer2.stem(\"having\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAs9XMxTszTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spanishStemmer = SnowballStemmer(\"spanish\", ignore_stopwords=True)\n",
        "spanishStemmer.stem(\"Corriendo\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc4Zm8AWs-pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmization reduces the inflected words properly ensuring that the root \n",
        "# word belongs to the language. A lemma (plural lemmas or lemmata) is the \n",
        "# cononical form, dictionary form, or citatino form of a set of words."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odYH7ciftarD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeplfNpstb6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations = \"?:!.,;\"\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "  if word in punctuations:\n",
        "    sentence_words.remove(word)\n",
        "\n",
        "\n",
        "sentence_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvCxFgVcuWtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
        "for word in sentence_words:\n",
        "  print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoHFrWqfvcMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you need to provide the context in which you want to lemmatize that is\n",
        "# the parts-of-speech (POS). This is done by giving the value for pos\n",
        "# parameter in wordnet_lemmatizer.lemmatize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKXkyjXKuywD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in sentence_words:\n",
        "  print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHcPEqabvYrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}