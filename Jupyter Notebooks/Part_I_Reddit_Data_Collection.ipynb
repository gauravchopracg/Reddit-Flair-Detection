{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part I - Reddit Data Collection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utiU6hxKFx7b",
        "colab_type": "text"
      },
      "source": [
        "Reddit Data Collection:\n",
        "===\n",
        "This part has been divided to answer question like:\n",
        "* How to collect data from reddit\n",
        "* Which features to extract from subreddit post (which will shape our future decision of building a Reddit Flair Detector and deploying it as a web service)\n",
        "* What we should consider while choosing a particular API to scrape reddit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPlKxj_KDz3z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##1. PRAW API:-\n",
        "PRAW, an acronym for \"Python Reddit API Wrapper\", is a python package that allows for simple access to reddit's API. An API is a gateway through which user can access and modify data of any website.\n",
        "PRAW has some great features which makes it easy-to-use for beginners like:\n",
        "  * simpler interface\n",
        "  * little to no skill required to use\n",
        "\n",
        "However, it has some limitations like:-\n",
        "  * you can fetch only upto 1000 submission or posts from reddit\n",
        "  * PRAW has a [lazy object](https://praw.readthedocs.org/en/stable/pages/lazy-loading.html) model, so it won't make any more requests than it needs to which means that in fetching comments, it makes a new request for each username that going to take hours to collect.\n",
        "\n",
        "Other than that, it requires user to create an account in reddit, sign up as developer, create an application and then provide credentials for obtaining data.\n",
        "\n",
        "Enough taking let's get starting on how to use it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJmwbJvhEcnU",
        "colab_type": "text"
      },
      "source": [
        "### How to use:\n",
        "The very first thing you'll need to do is \"Create an App\" within Reddit to get the OAuth2 keys to access the API. \n",
        "\n",
        "Go to [this page](https://www.reddit.com/prefs/apps) and click create app or create another app button at the bottom left\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*GQ8IREDENnkCRQT3VS55mQ.png\">\n",
        "\n",
        "This will open a form where you need to fill in a name, description and redirect uri. For the redirect uri you should choose https://localhost:8080 as described in the excellent [PRAW documentation](https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*ssLYczSLGzfm6SPM7mWzBg.png\">\n",
        "\n",
        "After pressing create app a new application will appear. Here you can find the authentication informtion needed to create the prew.Reddit instance.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*khszOCCaCtqZ6jM19uhpiQ.png\">\n",
        "\n",
        "Now, you can use this information to authenticate yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kOwr0l8g4fK",
        "colab_type": "code",
        "outputId": "24efb048-859a-4959-bafc-eff4d0914b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# PRAW can be installed using python package installer\n",
        "!pip install praw"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/39/17251486951815d4514e4a3f179d4f3e7af5f7b1ce8eaba5a3ea61bc91f2/praw-7.0.0-py3-none-any.whl (143kB)\n",
            "\r\u001b[K     |██▎                             | 10kB 22.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 81kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 2.8MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.16\n",
            "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 8.7MB/s \n",
            "\u001b[?25hCollecting prawcore<2.0,>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/8e/d076cb8f26523f91eef3e75d6cf9143b2f16d67ce7d681a61d0bbc783f49/prawcore-1.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.16->praw) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.8)\n",
            "Installing collected packages: update-checker, websocket-client, prawcore, praw\n",
            "Successfully installed praw-7.0.0 prawcore-1.3.0 update-checker-0.16 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyxPOvY0hFUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import library\n",
        "import praw\n",
        "\n",
        "# Before it can be used to scrape data we need to authenticate ourselves.\n",
        "# Use the information from the above section to know how to do it.\n",
        "# For this we need to create a Reddit instance and provide it with client_id,\n",
        "# client_secret, user_agent, username and password\n",
        "reddit = praw.Reddit(client_id='PERSONAL_USE_SCRIPT_14_CHARS', \\\n",
        "                     client_secret='SECRET_KEY_27_CHARS ', \\\n",
        "                     user_agent='YOUR_APP_NAME', \\\n",
        "                     username='YOUR_REDDIT_USER_NAME', \\\n",
        "                     password='YOUR_REDDIT_LOGIN_PASSWORD')\n",
        "\n",
        "# get post from india subreddit\n",
        "india_subreddit = reddit.subreddit('india')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNSe-o6PHgg-",
        "colab_type": "text"
      },
      "source": [
        "If you visit any post in [r/india](https://www.reddit.com/r/india/) subreddit, the topics in r/india subreddit are: AskIndia, Non-Political, Scheduled, Photography, Science/Technology, Politics, Business/Finance, Policy/Economy,Sports, Food, Coronavirus. \n",
        "\n",
        "However, to generalize machine learning algorithms or neural network millions of observation are required; that's why I have used flairs:\n",
        "**AskIndia, Non-Political, Politics, Policy/Economy, Sports, Food, Science/Technology, Business/Finance, Photography**\n",
        "\n",
        "*These are the most common flairs that exist an year ago and also exist now in [r/india](https://www.reddit.com/r/india/) subreddit*\n",
        "\n",
        "You can observe that there are 24 attributes of a subreddit post as discussed in [PRAW documentation](https://praw.readthedocs.io/en/latest/code_overview/models/submission.html).For Reddit flair detection task, I have only considered ones that are related to flair of the post.\n",
        "\n",
        "**Note: This is the hypothesis of data collection that we will try to verify in EDA section**\n",
        "\n",
        "These are given in following table: \n",
        "\n",
        "| Attribute | Description |\n",
        "| --- | --- |\n",
        "| author | Author of that post (Some people only post political content, some post Science/Technology content). |\n",
        "| comments | Comments on that particular post relates to Language Modelling.  |\n",
        "| created_utc | Time the submission was created, represented in Unix Time. (This might relate to people post religious content in morning and political in afternoon) |\n",
        "| link_flair_text | The flair of that particular postThe link flair's text content, or None if not flaired. |\n",
        "| num_comments | The number of comments on that post. |\n",
        "| over_18 | Whether or not the submission has been marked as NSFW. |\n",
        "| score | The number of upvotes for the submission. |\n",
        "| selftext | The submission selftext - an empty string if a link post. |\n",
        "| title | The title of the post. |\n",
        "| url | The URL the submission links to, or the permalink if a selfpost. |\n",
        "| comments_authors | author of comments appended together|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1J43mjNhvYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# to calculate time taken to scrape reddit\n",
        "\n",
        "# create an empty python list to append posts from subreddit\n",
        "posts = []\n",
        "\n",
        "# a python list of all the flairs to collect post from corresponding tags\n",
        "# These will be the labels in classification.\n",
        "flairs = ['AskIndia','Non-Political','Politics','Policy/Economy','Sports','Food','Science/Technology','Business/Finance','Photography']\n",
        "\n",
        "# iterate through each flair\n",
        "for flair in flairs:\n",
        "  \n",
        "  # collect relevant posts by searching in subreddit (less than 100)\n",
        "  relevant_subreddits = subreddit.search(f\"flair_name:{flair}\", limit=100)\n",
        "  \n",
        "  # iterate through each post\n",
        "  for submission in relevant_subreddits:\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    comment=''\n",
        "    authors=''\n",
        "    count=0\n",
        "    for top_level_comment in submission.comments:\n",
        "      # join all comments on post\n",
        "      comment = comment + ' ' + top_level_comment.body\n",
        "      # join all authors of comments on post\n",
        "      authors = authors + ' ' + str(top_level_comment.author)\n",
        "\n",
        "      count+= 1\n",
        "      \n",
        "      if (count>10):\n",
        "        break\n",
        "\n",
        "    posts.append([submission.author, submission.created_utc, submission.link_flair_text, submission.num_comments, submission.score, submission.over_18, submission.selftext, submission.title, submission.url, comment, authors])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1HWLXbspvqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd # for data preprocessing and manipulation\n",
        "\n",
        "# transform list of dictionary to pandas dataframe for easier preprocessing\n",
        "data = pd.DataFrame(posts, columns = ['author', 'created_utc', 'link_flair_text', 'num_comments', 'score', 'over_18', 'selftext', 'title', 'url', 'comment', 'authors'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTF-fRFh5Q5Z",
        "colab_type": "code",
        "outputId": "044a2d01-c6de-4e29-8f18-12bd476c1c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "# a look at data\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>link_flair_text</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>over_18</th>\n",
              "      <th>selftext</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>comment</th>\n",
              "      <th>authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sanand_satwik</td>\n",
              "      <td>1.586713e+09</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>134</td>\n",
              "      <td>1047</td>\n",
              "      <td>False</td>\n",
              "      <td>Hi....It's really tough time for everyone. I r...</td>\n",
              "      <td>Lost my Job, Sick Mother and Paralysed Dad, In...</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/g014wc...</td>\n",
              "      <td>I'm a freelancer. Don't listen to the idiots ...</td>\n",
              "      <td>hashedram diabapp xataari Aashayrao sarcrasti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TWO-WHEELER-MAFIA</td>\n",
              "      <td>1.586419e+09</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>204</td>\n",
              "      <td>648</td>\n",
              "      <td>False</td>\n",
              "      <td>We have floods, terrorist attacks, famines due...</td>\n",
              "      <td>Why does the government come with a begging bo...</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/fxofyu...</td>\n",
              "      <td>I don't understand why they don't use money f...</td>\n",
              "      <td>Kinky-Monk ak32009 fools_eye None DwncstSheep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GauGau24</td>\n",
              "      <td>1.587355e+09</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>115</td>\n",
              "      <td>158</td>\n",
              "      <td>False</td>\n",
              "      <td>I don't think we've spend so much time with fa...</td>\n",
              "      <td>People stuck with their family during the lock...</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/g4lrhm...</td>\n",
              "      <td>yesterday we had a major fight. (me and my wi...</td>\n",
              "      <td>Best-Economist Srthak_ ppccbba tb33296 damnji...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Oomada9</td>\n",
              "      <td>1.587672e+09</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>110</td>\n",
              "      <td>103</td>\n",
              "      <td>False</td>\n",
              "      <td>Does caste still exist in India? Do people sti...</td>\n",
              "      <td>How prominent is the caste system in India now...</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/g6tldd...</td>\n",
              "      <td>Very.  \\n\\n\\nHad a very good friend who was i...</td>\n",
              "      <td>merlin318 Vpee26 ppccbba Cierno Buns4Funz Sel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>indianoogler</td>\n",
              "      <td>1.586178e+09</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>206</td>\n",
              "      <td>266</td>\n",
              "      <td>False</td>\n",
              "      <td>The corona virus has given me some time to thi...</td>\n",
              "      <td>Men who are 30+ and have decided not to get ma...</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/fvy95j...</td>\n",
              "      <td>Plan your finances. Work and enjoy in your ow...</td>\n",
              "      <td>RedDevil-84 khushraho kingof-potatos congrats...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              author  ...                                            authors\n",
              "0      sanand_satwik  ...   hashedram diabapp xataari Aashayrao sarcrasti...\n",
              "1  TWO-WHEELER-MAFIA  ...   Kinky-Monk ak32009 fools_eye None DwncstSheep...\n",
              "2           GauGau24  ...   Best-Economist Srthak_ ppccbba tb33296 damnji...\n",
              "3            Oomada9  ...   merlin318 Vpee26 ppccbba Cierno Buns4Funz Sel...\n",
              "4       indianoogler  ...   RedDevil-84 khushraho kingof-potatos congrats...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZok2z3MMUzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for further preprocessing, transform it into csv file as it saves time \n",
        "# and we don't have to go through same process again.\n",
        "data.to_csv('data.csv', index=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE5hs-QiMcTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only works on Google Colab\n",
        "from google.colab import files\n",
        "# download data on personal computer\n",
        "files.download('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpSsTfITRhEW",
        "colab_type": "text"
      },
      "source": [
        "### Might want to consider:\n",
        "If you are planning on using PRAW API for Reddit Data Collection, it has a limitation of sending new request for fetching comments. In the above block, It took **7 hours to collect 1000 post from 10 flairs (with 10 comments from each post)**. One way to overcome this problem is to use multiprocessing or Pushshift's API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiCbSOB-qGm2",
        "colab_type": "text"
      },
      "source": [
        "##2. Pushshift's API :- \n",
        "PRAW is the main Reddit API used for extracting data from the site using Python. Although there are a few limitations including extracting submissions between specific dates and you can only extract 1000 submission from a subreddit. This inconvenience was overcome by Pushshift's API for accessing Reddit's data.\n",
        "\n",
        "\n",
        "**The Pushshift API**\n",
        "\n",
        "Pushshift is a big-data storage and analytics project started and maintained by Jason Baumgartner. The Pushshift API serves a copy of reddit objects. Currently, data is copied into Pushshift at the time it is posted to reddit. Therefore, scores and other meta such as edits to a submission's selftext or a comment's body field may not reflect what is displayed by reddit. A future version of the API will update data at timed intervals.\n",
        "\n",
        "But it has some great features like:\n",
        "  * access the subreddit Data without even needing Reddit credentials.\n",
        "  * analyze large quantities of reddit data\n",
        "  * grab data for a specific data range in the past\n",
        "  * search for comments\n",
        "  * aggregate data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP3vUc6zQrgV",
        "colab_type": "text"
      },
      "source": [
        "### How to use\n",
        "We can access the Pushshift API through building an URL with the relevant parameters without even needing Reddit credentials.\n",
        "\n",
        "Without parameters, this is the foundation of the URL you'll use to access Redit: https://api.pushshift.io/reddit/search/\n",
        "\n",
        "Now with parameters, This is the url which will access india subreddit between 2 dates written in unix timestamps and search for all submission that contain the keyword - **coronavirus**: https://api.pushshift.io/reddit/search/submission/?q=coronavirus&after=1514764800&before=1517443200&subreddit=india\n",
        "\n",
        "So, this will be the template in which we can insert dates and keyword to search for specific posts: https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
        "\n",
        "**To make sure we are getting everything for the specific time period**\n",
        "\n",
        "we can crate a method for building time period search intervals. Add in logic to request more posts. We will pull the last created on timestamp prior to the next request.\n",
        "\n",
        "**NOTE: This method has a downside that it can fetch duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-0TP11FQq2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the relevant modules\n",
        "import pandas as pd # for reading csv file and data manipulation\n",
        "import requests # to access the Pushshift API through building an URL\n",
        "import json # to transform web page data into json format\n",
        "import csv # to upload into a CSV for further analysis\n",
        "import time # for transformation of timestamps\n",
        "import datetime # for transformation of timestamps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3qDee10Un_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subreddit to query\n",
        "sub = 'india'\n",
        "# before and after dates\n",
        "before = \"1577836800\" #01/01/2020 @ 12:00am (UTC)\n",
        "after = \"1546300800\"  #01/01/2019 @ 12:00am (UTC)\n",
        "query = \"state\" # to search in subreddit\n",
        "subCount = 0\n",
        "subStats = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pL6In-vUz45",
        "colab_type": "code",
        "outputId": "fd18f81e-aefc-4507-bfa9-dcabb738e3e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We can access the Pushshift API through building an URL with the relevant\n",
        "# parameters without even needing Reddit credentials\n",
        "url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
        "# to know how does the data look like click below pushshift URL with\n",
        "# the parameters to see for yourself\n",
        "url"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://api.pushshift.io/reddit/search/submission/?title=state&size=1000&after=1546300800&before=1577836800&subreddit=india'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvvtjpzOVZwv",
        "colab_type": "code",
        "outputId": "7a08f79c-c5dc-49e4-98b8-c968a067ac96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "r = requests.get(url) # Requests module is used to access the URL  \n",
        "data = json.loads(r.text) # with the JSON module collecting the text version of the page\n",
        "dat = data['data']\n",
        "dat[0] # a look at first post"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'author': 'TheEternalGentleman',\n",
              " 'author_flair_css_class': None,\n",
              " 'author_flair_richtext': [],\n",
              " 'author_flair_text': None,\n",
              " 'author_flair_type': 'text',\n",
              " 'author_fullname': 't2_13flz4',\n",
              " 'author_patreon_flair': False,\n",
              " 'can_mod_post': False,\n",
              " 'contest_mode': False,\n",
              " 'created_utc': 1546328809,\n",
              " 'domain': 'newindianexpress.com',\n",
              " 'full_link': 'https://www.reddit.com/r/india/comments/abgc0m/in_about_2_hours_3_million_women_take_to_the/',\n",
              " 'gildings': {'gid_1': 0, 'gid_2': 0, 'gid_3': 0},\n",
              " 'id': 'abgc0m',\n",
              " 'is_crosspostable': True,\n",
              " 'is_meta': False,\n",
              " 'is_original_content': False,\n",
              " 'is_reddit_media_domain': False,\n",
              " 'is_robot_indexable': True,\n",
              " 'is_self': False,\n",
              " 'is_video': False,\n",
              " 'link_flair_background_color': '#ddbd37',\n",
              " 'link_flair_css_class': 'Politics',\n",
              " 'link_flair_richtext': [{'e': 'text', 't': 'Politics'}],\n",
              " 'link_flair_template_id': '77f04f12-7ea0-11e3-ac66-22000a0b8292',\n",
              " 'link_flair_text': 'Politics',\n",
              " 'link_flair_text_color': 'dark',\n",
              " 'link_flair_type': 'richtext',\n",
              " 'locked': False,\n",
              " 'media_only': False,\n",
              " 'no_follow': True,\n",
              " 'num_comments': 8,\n",
              " 'num_crossposts': 0,\n",
              " 'over_18': False,\n",
              " 'parent_whitelist_status': 'all_ads',\n",
              " 'permalink': '/r/india/comments/abgc0m/in_about_2_hours_3_million_women_take_to_the/',\n",
              " 'pinned': False,\n",
              " 'post_hint': 'link',\n",
              " 'preview': {'enabled': False,\n",
              "  'images': [{'id': 'tqmt4CAx5pZ9FuqEg4qr_hGUfJpTHzGwGbC1_hX8c6I',\n",
              "    'resolutions': [{'height': 70,\n",
              "      'url': 'https://external-preview.redd.it/9EkY5S7PsMhm-Oeh9W7JejRB8PQ_WYoeinXB9e2Tt4w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f414744000265df4adb4fabcae5ced5f10fb1791',\n",
              "      'width': 108},\n",
              "     {'height': 140,\n",
              "      'url': 'https://external-preview.redd.it/9EkY5S7PsMhm-Oeh9W7JejRB8PQ_WYoeinXB9e2Tt4w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=349eb30335abc2811a496eaa1a3e5c6402fc4a64',\n",
              "      'width': 216},\n",
              "     {'height': 208,\n",
              "      'url': 'https://external-preview.redd.it/9EkY5S7PsMhm-Oeh9W7JejRB8PQ_WYoeinXB9e2Tt4w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=291e3be553c8a124ba73aa6e2d5059c7055482a5',\n",
              "      'width': 320}],\n",
              "    'source': {'height': 390,\n",
              "     'url': 'https://external-preview.redd.it/9EkY5S7PsMhm-Oeh9W7JejRB8PQ_WYoeinXB9e2Tt4w.jpg?auto=webp&amp;s=f3358e263d69a0d25ca8bbf81f8329041e2443da',\n",
              "     'width': 600},\n",
              "    'variants': {}}]},\n",
              " 'pwls': 6,\n",
              " 'retrieved_on': 1546328810,\n",
              " 'score': 1,\n",
              " 'selftext': '',\n",
              " 'send_replies': True,\n",
              " 'spoiler': False,\n",
              " 'stickied': False,\n",
              " 'subreddit': 'india',\n",
              " 'subreddit_id': 't5_2qh1q',\n",
              " 'subreddit_subscribers': 171916,\n",
              " 'subreddit_type': 'public',\n",
              " 'thumbnail': 'https://b.thumbs.redditmedia.com/HwnpN6p_zQbgl7PF5FMlpM2Zx4Fxrd7emYXovdIGAbw.jpg',\n",
              " 'thumbnail_height': 91,\n",
              " 'thumbnail_width': 140,\n",
              " 'title': 'In about 2 hours, 3 million women take to the streets of Kerala, to form a human wall around 620 km long from tip to toe of the state.',\n",
              " 'url': 'http://www.newindianexpress.com/states/kerala/2018/dec/31/all-you-need-to-know-about-the-womens-wall-campaign-in-kerala-1918852.html',\n",
              " 'whitelist_status': 'all_ads',\n",
              " 'wls': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOSNtyKHViyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPushshiftData(query, after, before, sub):\n",
        "    '''\n",
        "    Function to transform web page data into json\n",
        "    '''\n",
        "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
        "    print(url)\n",
        "    r = requests.get(url) # Requests module is used to access the URL\n",
        "    data = json.loads(r.text) # with the JSON module collecting the text version of the page\n",
        "    return data['data']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5r4P5YvXP0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# similarly we can build another function to extract key data points:\n",
        "def collectSubData(subm):\n",
        "    subData = list() #list to store data points\n",
        "    title = subm['title'] # title of the subreddit post\n",
        "    url = subm['url'] # url associated with it\n",
        "    try:\n",
        "        flair = subm['link_flair_text'] # flair if given which it belongs to\n",
        "    except KeyError:\n",
        "        flair = \"NaN\"  # else none\n",
        "    author = subm['author'] # author of post\n",
        "    sub_id = subm['id'] # unique identifier of post\n",
        "    score = subm['score'] # number of upvotes\n",
        "    created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
        "    numComms = subm['num_comments'] # number of comments\n",
        "    permalink = subm['permalink'] # permalink of post\n",
        "    \n",
        "    subData.append((sub_id,title,url,author,score,created,numComms,permalink,flair)) # adding it a single python list\n",
        "    subStats[sub_id] = subData"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nddB04aSeBGN",
        "colab_type": "code",
        "outputId": "e5d798ab-6b54-4819-cf21-c5e194722898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# now we can run code and loop until all submission are collected from \n",
        "# a subreddit\n",
        "data = getPushshiftData(query, after, before, sub)\n",
        "# Will run until all posts have been gathered \n",
        "# from the 'after' date up until before date\n",
        "while len(data) > 0:\n",
        "    for submission in data:\n",
        "        collectSubData(submission)\n",
        "        subCount+=1\n",
        "    # Calls getPushshiftData() with the created date of the last submission\n",
        "    print(len(data))\n",
        "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
        "    after = data[-1]['created_utc']\n",
        "    data = getPushshiftData(query, after, before, sub)\n",
        "    \n",
        "print(len(data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://api.pushshift.io/reddit/search/submission/?title=state&size=1000&after=1546300800&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-06 14:25:28\n",
            "https://api.pushshift.io/reddit/search/submission/?title=state&size=1000&after=1567779928&before=1577836800&subreddit=india\n",
            "506\n",
            "2019-12-31 12:57:44\n",
            "https://api.pushshift.io/reddit/search/submission/?title=state&size=1000&after=1577797064&before=1577836800&subreddit=india\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--IdywdjeA-b",
        "colab_type": "code",
        "outputId": "e3585ff0-cf65-413a-dd06-f1889cb92a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# sanity check to make sure we have our data for futher analysis\n",
        "print(str(len(subStats)) + \" submissions have added to list\")\n",
        "print(\"1st entry is:\")\n",
        "print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
        "print(\"Last entry is:\")\n",
        "print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1506 submissions have added to list\n",
            "1st entry is:\n",
            "In about 2 hours, 3 million women take to the streets of Kerala, to form a human wall around 620 km long from tip to toe of the state. created: 2019-01-01 07:46:49\n",
            "Last entry is:\n",
            "Pakistan uses terrorism as tool of state policy: Army Chief Naravane created: 2019-12-31 12:57:44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_huu-ceTYO99",
        "colab_type": "code",
        "outputId": "d0dc6bc3-fb75-4971-cc23-69fd07ea30c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# upload to csv file\n",
        "def updateSubs_file():\n",
        "    '''\n",
        "    function to store data into a csv file\n",
        "    '''\n",
        "    upload_count = 0\n",
        "    # directory of google colaboratory\n",
        "    location = \"\\\\Reddit Data\\\\\"\n",
        "    print(\"input filename of submission file, please add .csv\")\n",
        "    filename = input()\n",
        "    file = location + filename\n",
        "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
        "        a = csv.writer(file, delimiter=',')\n",
        "        headers = [\"Post ID\",\"Title\",\"Url\",\"Author\",\"Score\",\"Publish Date\",\"Total No. of Comments\",\"Permalink\",\"Flair\"]\n",
        "        a.writerow(headers)\n",
        "        for sub in subStats:\n",
        "            a.writerow(subStats[sub][0])\n",
        "            upload_count+=1\n",
        "            \n",
        "        print(str(upload_count) + \" submissions have been uploaded\") # print updates while writing into csv file\n",
        "updateSubs_file()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input filename of submission file, please add .csv\n",
            "submission.csv\n",
            "1506 submissions have been uploaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eNAHUtuJ-Zc",
        "colab_type": "text"
      },
      "source": [
        "Approach:\n",
        "---\n",
        "In this notebook, I have explored different ways to scrape Reddit with their own limitations. The aim of this Part is explore different libraries to collect data, recognizing their limitations and choosing one suitable for our task i.e., Reddit Flair Detection.\n",
        "\n",
        "As, our task requires us to building a Reddit Flair Detection classifier and deploying it to heroku. Since, we are going to test different machine learning models and deep learning algorithms it will be beneficial if we have millions of observations on dataset. Therefore, I have collected **two dataset** one from Reddit API and another from Pushshift's API\n",
        "This dataset is from **1st January 2019 to 1st January 2020** containing title and body of reddit posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFy-ZbTzAgV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def getPushshiftData(after, before):\n",
        "    sub = \"india\"\n",
        "    url = 'https://api.pushshift.io/reddit/search/submission/?size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
        "    print(url)\n",
        "    r = requests.get(url)\n",
        "    data = json.loads(r.text)\n",
        "    return data['data']\n",
        "\n",
        "def collectSubData(subm):\n",
        "    subData = list() #list to store data points\n",
        "    sub_id = subm['id']\n",
        "    title = subm['title']  \n",
        "    \n",
        "    try:\n",
        "        body = subm['selftext']\n",
        "    except KeyError:\n",
        "        body = \"\" \n",
        "    \n",
        "    try:\n",
        "        flair = subm['link_flair_text']\n",
        "        subData.append((title,body,flair))\n",
        "        subStats[sub_id] = subData\n",
        "    except KeyError:\n",
        "        flair = \"NaN\" \n",
        "\n",
        "        \n",
        "#before and after dates\n",
        "before = \"1577836800\" #01/01/2020 @ 12:00am (UTC)\n",
        "after = \"1546300800\"  #01/01/2019 @ 12:00am (UTC)\n",
        "subCount = 0\n",
        "subStats = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFM1ZbofBAIm",
        "colab_type": "code",
        "outputId": "7564895c-3112-4887-855b-cab2e27b77c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data = getPushshiftData(after, before)\n",
        "\n",
        "# Will run until all posts have been gathered \n",
        "# from the 'after' date up until before date\n",
        "while len(data) > 0:\n",
        "    for submission in data:\n",
        "        collectSubData(submission)\n",
        "        subCount+=1\n",
        "    # Calls getPushshiftData() with the created date of the last submission\n",
        "    print(len(data))\n",
        "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
        "    after = data[-1]['created_utc']\n",
        "    data = getPushshiftData(after, before)\n",
        "    \n",
        "print(len(data))\n",
        "\n",
        "subStats = list(subStats.values())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546300800&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-03 02:43:12\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546483392&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-04 17:25:08\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546622708&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-07 05:46:27\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546839987&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-09 03:16:49\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547003809&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-10 17:53:53\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547142833&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-12 21:01:08\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547326868&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-15 06:07:07\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547532427&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-17 06:07:33\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547705253&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-19 07:48:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547884098&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-21 14:22:19\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548080539&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-23 15:07:24\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548256044&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-25 16:23:08\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548433388&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-28 06:37:16\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548657436&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-01-30 06:26:07\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548829567&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-01 03:00:07\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548990007&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-02 20:19:17\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1549138757&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-05 04:43:07\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1549341787&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-07 04:08:40\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1549512520&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-08 19:38:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1549654708&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-11 06:41:31\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1549867291&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-13 05:58:56\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550037536&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-14 20:36:37\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550176597&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-16 14:55:43\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550328943&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-18 13:25:04\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550496304&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-20 07:58:16\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550649496&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-22 03:50:49\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550807449&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-23 20:58:15\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550955495&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-25 22:18:12\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551133092&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-27 07:42:35\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551253355&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-02-28 09:41:16\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551346876&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-01 14:28:17\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551450497&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-03 10:14:23\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551608063&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-05 05:30:43\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551763843&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-06 14:30:15\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551882615&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-08 07:50:40\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1552031440&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-10 06:59:53\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1552201193&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-12 04:27:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1552364848&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-13 09:30:49\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1552469449&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-14 17:31:37\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1552584697&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-16 13:10:49\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1552741849&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-18 09:33:58\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1552901638&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-20 05:26:01\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1553059561&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-21 16:21:54\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1553185314&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-23 12:59:39\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1553345979&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-25 15:14:36\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1553526876&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-27 11:00:02\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1553684402&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-29 06:45:03\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1553841903&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-03-31 05:47:35\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1554011255&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-01 17:58:08\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1554141488&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-03 12:36:59\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1554295019&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-05 08:53:23\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1554454403&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-07 07:53:40\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1554623620&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-09 06:14:10\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1554790450&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-10 16:43:35\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1554914615&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-12 08:26:09\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1555057569&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-14 06:18:04\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1555222684&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-16 06:13:06\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1555395186&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-17 17:46:34\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1555523194&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-19 12:47:11\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1555678031&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-21 12:29:33\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1555849773&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-23 09:40:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1556012428&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-25 07:35:13\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1556177713&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-27 03:53:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1556337208&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-29 06:02:03\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1556517723&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-04-30 19:12:30\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1556651550&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-02 15:43:37\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1556811817&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-04 13:27:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1556976438&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-06 13:31:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1557149488&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-08 11:31:16\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1557315076&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-10 07:32:48\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1557473568&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-12 05:26:32\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1557638792&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-14 04:25:23\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1557807923&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-15 19:12:36\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1557947556&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-17 12:56:31\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1558097791&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-19 15:22:11\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1558279331&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-21 12:53:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1558443198&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-23 07:04:38\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1558595078&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-24 13:21:29\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1558704089&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-26 13:46:54\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1558878414&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-28 09:34:17\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559036057&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-30 02:53:20\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559184800&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-05-31 17:03:16\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559322196&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-03 03:48:26\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559533706&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-04 18:58:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559674698&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-06 16:21:26\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559838086&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-08 14:24:07\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1560003847&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-10 15:01:25\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1560178885&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-12 07:42:55\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1560325375&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-14 03:18:23\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1560482303&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-16 03:33:40\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1560656020&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-18 02:38:37\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1560825517&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-19 16:13:16\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1560960796&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-21 11:58:12\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1561118292&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-23 12:37:30\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1561293450&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-25 07:50:07\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1561449007&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-27 05:19:36\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1561612776&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-06-28 18:42:51\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1561747371&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-01 04:00:04\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1561953604&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-03 01:08:07\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562116087&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-04 15:21:33\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562253693&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-06 20:04:56\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562443496&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-09 03:50:40\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562644240&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-10 19:37:15\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562787435&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-12 18:16:39\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562955399&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-15 05:48:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1563169698&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-17 04:50:43\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1563339043&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-19 12:33:40\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1563539620&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-22 02:37:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1563763038&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-24 03:36:24\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1563939384&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-25 22:40:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1564094428&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-28 03:50:36\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1564285836&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-30 03:26:20\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1564457180&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-07-31 17:57:22\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1564595842&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-02 19:01:04\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1564772464&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-05 04:52:08\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1564980728&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-06 17:26:42\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1565112402&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-08 16:47:05\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1565282825&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-10 16:24:55\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1565454295&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-12 17:23:12\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1565630592&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-14 15:38:48\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1565797128&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-16 12:56:27\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1565960187&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-18 16:14:56\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1566144896&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-20 16:29:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1566318568&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-22 17:38:12\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1566495492&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-24 17:28:59\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1566667739&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-27 04:57:03\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1566881823&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-29 05:06:09\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1567055169&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-08-31 08:33:34\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1567240414&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-02 13:05:36\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1567429536&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-04 13:41:59\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1567604519&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-06 17:14:01\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1567790041&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-08 21:38:03\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1567978683&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-10 19:31:25\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1568143885&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-13 01:21:11\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1568337671&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-15 08:32:28\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1568536348&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-17 12:40:20\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1568724020&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-19 08:56:50\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1568883410&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-21 05:37:26\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1569044246&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-23 09:46:19\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1569231979&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-25 08:41:09\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1569400869&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-27 05:20:03\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1569561603&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-09-29 09:17:36\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1569748656&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-01 12:17:11\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1569932231&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-03 09:40:10\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1570095610&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-05 13:04:34\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1570280674&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-07 22:50:38\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1570488638&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-10 01:32:27\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1570671147&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-11 18:19:10\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1570817950&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-14 06:21:12\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1571034072&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-16 04:39:52\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1571200792&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-18 05:26:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1571376378&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-20 08:19:29\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1571559569&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-22 11:13:45\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1571742825&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-24 14:32:33\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1571927553&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-26 23:52:36\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1572133956&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-29 07:07:08\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1572332828&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-10-31 04:24:09\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1572495849&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-02 19:04:49\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1572721489&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-05 07:40:06\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1572939606&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-07 11:08:37\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1573124917&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-09 16:33:51\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1573317231&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-12 11:36:02\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1573558562&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-15 08:56:47\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1573808207&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-18 07:19:17\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1574061557&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-21 02:38:50\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1574303930&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-23 14:25:26\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1574519126&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-26 12:53:27\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1574772807&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-11-29 05:21:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1575004878&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-01 15:24:00\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1575213840&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-04 06:38:36\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1575441516&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-06 12:20:08\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1575634808&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-09 08:52:48\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1575881568&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-11 13:38:00\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1576071480&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-13 13:09:21\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1576242561&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-15 19:08:50\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1576436930&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-16 21:07:21\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1576530441&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-18 05:49:06\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1576648146&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-19 09:25:47\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1576747547&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-20 05:56:43\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1576821403&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-21 07:41:32\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1576914092&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-22 14:42:36\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1577025756&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-24 06:38:52\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1577169532&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-26 06:02:37\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1577340157&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-28 02:30:48\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1577500248&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-30 06:40:33\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1577688033&before=1577836800&subreddit=india\n",
            "1000\n",
            "2019-12-31 19:36:26\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1577820986&before=1577836800&subreddit=india\n",
            "39\n",
            "2019-12-31 23:55:05\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1577836505&before=1577836800&subreddit=india\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzwMEQrqBCbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allowed_tags = ['AskIndia','Non-Political','Politics','Policy/Economy','Sports','Food','Science/Technology','Business/Finance','Photography']\n",
        "\n",
        "extracted_dict = {\n",
        "    \"TITLE\":[], \n",
        "    \"BODY\":[], \n",
        "    \"FLAIR\":[], \n",
        "}\n",
        "\n",
        "for sub in subStats:\n",
        "    if sub[0][2] not in allowed_tags:\n",
        "        continue\n",
        "    else:\n",
        "      extracted_dict[\"FLAIR\"].append(sub[0][2])\n",
        "    \n",
        "    extracted_dict[\"TITLE\"].append(sub[0][0].replace(\",\",\" \"))\n",
        "    extracted_dict[\"BODY\"].append(sub[0][1].replace(\",\",\" \"))\n",
        "    \n",
        "    \n",
        "\n",
        "pandas_data = pd.DataFrame(extracted_dict)\n",
        "#print(pandas_data)\n",
        "\n",
        "pandas_data.to_csv('data_1.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aON7BL3QDkuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('data_1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn6IDHxVSlwC",
        "colab_type": "text"
      },
      "source": [
        "**As in this part, We have collected two dataset one containing random post from 9 flairs and other containing an year data for 9 flairs. In EDA, we will explore both of them to find machine learning patterns**"
      ]
    }
  ]
}